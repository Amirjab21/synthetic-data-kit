# Master configuration file for Synthetic Data Kit

# Global paths configuration
paths:
  # Input data location (directory containing files to process)
  input: "data/input" # Directory containing PDF, HTML, DOCX, PPT, TXT files

  # Output locations (4-stage pipeline directories)
  output:
    parsed: "data/parsed" # Stage 1: Where parsed text files are saved (ingest output)
    generated: "data/generated" # Stage 2: Where generated QA pairs are saved (create output)
    curated: "data/curated" # Stage 3: Where curated QA pairs are saved (curate output)
    final: "data/final" # Stage 4: Where final training formats are saved (save-as output)

# LLM Provider configuration
llm:
  # Provider selection: "vllm" or "api-endpoint"
  provider: "api-endpoint"

# VLLM server configuration
vllm:
  api_base: "http://localhost:11434/api/generate" # Base URL for VLLM API
  port: 11434 # Port for VLLM server
  model: "llama3.2:3b" # Default model to use
  max_retries: 3 # Number of retries for API calls
  retry_delay: 1.0 # Initial delay between retries (seconds)

# API endpoint configuration
api-endpoint:
  api_base: "https://api.studio.nebius.com/v1/" # Optional base URL for API endpoint (null for default API)
  api_key: "eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwMzU0NDg4NTQ4MDIzNzQwMjgxMiIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkxMDc5NTc0NywidXVpZCI6ImMyZGQyODNlLTAzMmUtNGM0Yy1iNmE5LTY4MjM1YTcwYTNhMCIsIm5hbWUiOiJhbWlyIiwiZXhwaXJlc19hdCI6IjIwMzAtMDctMjBUMTY6MzU6NDcrMDAwMCJ9.xl2lS8nsTjClssk3F5OijUnM4PXVlb6eHlTvwhNEBx8" # API key for API endpoint or compatible service (can use env var instead)
  model: "deepseek-ai/DeepSeek-V3" # Default model to use
  max_retries: 3 # Number of retries for API calls
  retry_delay: 1.0 # Initial delay between retries (seconds)

# Ingest configuration
ingest:
  default_format: "txt" # Default output format for parsed files
  youtube_captions: "auto" # Options: "auto", "manual" - caption preference

# LLM generation parameters
generation:
  temperature: 0.7 # Higher = more creative, lower = more deterministic
  top_p: 0.95 # Nucleus sampling parameter

  # Document processing strategy
  # "auto": choose based on document size, "single": force single call, "chunking": force chunking
  processing_strategy: "auto"
  single_call_max_size: 8000 # Documents smaller than this use single call processing

  # Chunking parameters (used for large documents)
  chunk_size: 1500 # Size of text chunks for processing large documents
  overlap: 1000 # Overlap between chunks to maintain context (prevents losing info at boundaries)

  # Model parameters
  max_tokens: 20000 # Maximum tokens in LLM responses

  # Content generation targets
  num_pairs: 5 # Default number of QA pairs to generate
  num_cot_examples: 5 # Default number of Chain of Thought examples to generate
  num_cot_enhance_examples: null # Maximum number of conversations to enhance (null = enhance all)

  # Batch processing
  batch_size: 1 # Number of requests to batch together (for create)

  # Quality settings
  enable_deduplication: false # Remove very similar questions/examples
  similarity_threshold: 0.8 # Threshold for considering items similar (0.0-1.0)

# Content curation parameters
curate:
  threshold: 7.0 # Default quality threshold (1-10)
  batch_size: 5 # Number of items per batch for rating (smaller batches for API stability)
  inference_batch: 5 # Number of batches to process at once with VLLM
  temperature: 0.1 # Temperature for rating (lower = more consistent)

# Format conversion parameters
format:
  default: "jsonl" # Default output format
  include_metadata: true # Include metadata in output files
  pretty_json: true # Use indentation in JSON output

# Prompts for different tasks
prompts:
  # Summary generation prompt
  summary: |
    Summarize this document in 3-5 sentences, focusing on the main topic and key concepts.

  # QA pair generation prompt
  qa_generation: |
   Create 7 question-answer pairs that resemble a therapist's conversation with a patient from the provided text for LLM training.

    Content rules:
    1. Questions must be something that a recovering addict would confess or say to their therapist.
    2. Answers must be directly supported by the text and be in the style of the text.
    3. Generate exactly 7 question-answer pairs.

    Output format (this is the ONLY thing you should output):
    {
    "qa_pairs": [
      {
        "question": "Question 1?",
        "answer": "Answer 1."
      },
      {
        "question": "Question 2?",
        "answer": "Answer 2."
      },
      {
        "question": "Question 3?",
        "answer": "Answer 3."
      }
    ]
    }

        
    # Example Input
    -----------
    Source text:
    Dishonesty was stitched into the fabric of my life from as early as I can remember. My parents lied constantly, dangling promises they never meant to keep. They’d say things like, “There won’t be any more shouting or violence. I won’t hit you again. I won’t hit your mother. It’ll be different this time.” But it never was. My mother once even told my father that I wasn’t his son—just to wound him after he’d beaten her senseless. Lies weren’t just survival tactics in my household; they were weapons. I learned early that deceit wasn’t just normal—it was strategic. Not everyone grows up with this level of dishonesty. Some people only start lying when addiction enters their life

    Output:
    {
    "qa_pairs": [
      {
        "question": "I grew up around so much lying that I don’t even know what honesty feels like anymore. Is that normal?",
        "answer": "It’s understandable you feel that way. When dishonesty is all you’ve known, it becomes part of your survival system. But honesty isn’t a fixed trait — it’s something you can relearn by creating safety and trust, first within yourself, then with others."
      },
      {
        "question": "My parents used to promise they’d change — that the shouting and hitting would stop — but it never did. Why does that still hurt me so much even after all these years?",
        "answer": "Because those broken promises weren’t just words; they shaped your sense of safety and love. When people you depended on couldn’t keep their word, it taught your nervous system that trust equals danger. Healing often starts by acknowledging that pain and learning to separate their patterns from your worth."
      },
      {
        "question": "I feel like lying became part of who I am. I don’t even notice when I’m doing it sometimes. How do I stop?",
        "answer": "That awareness is already a powerful first step. Many people who grew up around deceit learned to use it as protection — to avoid conflict, shame, or punishment. The goal isn’t to shame yourself for it but to notice why the lie feels safer than the truth in that moment. From there, we can start practicing small acts of honesty in low-risk situations to rebuild that muscle."
      }
      ... 4 more pairs ...
    ]
    }


    # Real Data

    Use the following text for your answer. Do not make anything up in your answer.


    Source text:
    {text}

    Output:


  # QA pair rating prompt
  qa_rating: |
    Rate each question-answer pair on a scale from 1-10, based on:
    - Accuracy (0-3): factual correctness
    - Relevance (0-2): relevance to content
    - Clarity (0-2): clear language
    - Usefulness (0-3): value for model learning

    YOU MUST RETURN A VALID JSON OBJECT OR ARRAY WITH THIS EXACT SCHEMA:
    {{
      "question": "Exact question text",
      "answer": "Exact answer text",
      "rating": 8
    }}

    OR FOR MULTIPLE PAIRS:
    [
      {{"question": "Q1", "answer": "A1", "rating": 8}},
      {{"question": "Q2", "answer": "A2", "rating": 9}}
    ]

    *** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE - NO EXPLANATION, NO MARKDOWN ***

    QA pairs to rate:
    {pairs}

  # Chain of Thought generation prompt
  cot_generation: |
    Create complex reasoning examples from this text that demonstrate chain-of-thought thinking.

    Each example should have:
    1. A challenging question that requires step-by-step reasoning
    2. Detailed reasoning steps that break down the problem
    3. A concise final answer

    Return JSON format only:

    [
      {{
        "question": "Complex question about the text?",
        "reasoning": "Step 1: First, I need to consider...\nStep 2: Then, I analyze...\nStep 3: Finally, I can conclude...",
        "answer": "Final answer based on the reasoning."
      }},
      {{
        "question": "Another complex question?",
        "reasoning": "Step 1: First, I'll analyze...\nStep 2: Next, I need to determine...\nStep 3: Based on this analysis...",
        "answer": "Final answer drawn from the reasoning."
      }}
    ]

    Text:
    {text}

  # Chain of Thought enhancement prompt
  cot_enhancement: |
    You are an expert reasoning assistant. Your task is to enhance the given conversations by adding chain-of-thought reasoning.

    For each conversation, add detailed step-by-step reasoning to the assistant's responses while preserving the original answer.

    {include_simple_steps} = Whether to add reasoning to simple responses too. If false, only add reasoning to complex responses.

    Return the enhanced conversations as a JSON array matching this format:
    [
      [
        {{"role": "system", "content": "System message"}},
        {{"role": "user", "content": "User question"}},
        {{"role": "assistant", "content": "Let me think through this step by step:\n\n1. First, I need to consider...\n2. Then...\n\nTherefore, [original answer]"}}
      ],
      [
        {{"role": "system", "content": "System message"}},
        {{"role": "user", "content": "Another user question"}},
        {{"role": "assistant", "content": "Let me work through this:\n\n1. I'll start by...\n2. Next...\n\nIn conclusion, [original answer]"}}
      ]
    ]

    Original conversations:
    {conversations}
